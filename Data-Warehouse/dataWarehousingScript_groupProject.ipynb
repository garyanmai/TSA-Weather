{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "88c2a5d3",
   "metadata": {
    "id": "88c2a5d3"
   },
   "source": [
    "# Analyzing TSA Throughput Data and Historical Weather Information\n",
    "- A Data Warehousing for Analytics undergraduate group project for CIS 4400 course.\n",
    "\n",
    "**Requirements**: https://docs.google.com/document/d/1djD0VMOxct1eiHj7tiv0HYmfNdgD4YXA1nGX4urJj_U/edit?usp=sharing\n",
    "\n",
    "**Link to TSA Data Set**: https://www.tsa.gov/foia/readingroom?page=1\n",
    "\n",
    "**Link to Weather API Data Set**: https://open-meteo.com/en/docs/historical-weather-api\n",
    "\n",
    "**Data Dictionary**: https://docs.google.com/spreadsheets/d/1IMv8EoMX21I7BfsayR2Qs0bNYVQoMm-qADgmQC2n_Bk/edit?usp=sharing\n",
    "\n",
    "# BigQuery Data Warehousing Script\n",
    "\n",
    "**Link To Data**: https://data.cityofnewyork.us/City-Government/NYC-Citywide-Annualized-Calendar-Sales-Update/w2pb-icbu\n",
    "\n",
    "**API end-point**: https://data.cityofnewyork.us/resource/w2pb-icbu.json\n",
    "\n",
    "**Data Dictionary**: https://data.cityofnewyork.us/api/views/w2pb-icbu/files/8ed811b4-8238-4b5e-9acc-1e33d8705498?download=true&filename=Annualized_Calendar_Sales_Update%20Data_Dictionary.xlsx\n",
    "\n",
    "**Cleaned Data Dictionary**: https://docs.google.com/spreadsheets/d/17XyGmnw2fZuTMCWVKB1XiWGHQuwqWOidm0w80lbIyjE/edit?usp=sharing\n",
    "\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "64bdae7f",
   "metadata": {
    "id": "64bdae7f"
   },
   "source": [
    "# Grant Required Permissions to your Google Cloud Service Account to Create a BigQuery Data Set"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "5d061580",
   "metadata": {
    "id": "5d061580"
   },
   "source": [
    "**1. Grant Permissions:**\n",
    "- Go to the Google Cloud Console.\n",
    "- Navigate to the IAM & Admin > IAM page.\n",
    "- Locate the user account associated with the credentials you are using.\n",
    "- Click \"ADD IAM CONDITION\"\n",
    "- Under the \"Role\" field, select the \"BigQuery Data Editor\"\n",
    "- Click \"SAVE\" to grant permissions.\n",
    "\n",
    "**Note: The BigQuery Data Editor role allows your service account access to edit all the contents of datasets. This step is important for loading your dataset from Google Cloud to the BigQuery Data Warehouse.**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "cb54749a",
   "metadata": {
    "id": "cb54749a"
   },
   "source": [
    "# Install the google-cloud-storage library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "663df168",
   "metadata": {
    "id": "663df168"
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "pip install google-cloud-storage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "jqvvAXzkQu6d",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jqvvAXzkQu6d",
    "outputId": "cf568b59-3582-464e-ad85-66483d7be7b5"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "\n",
    "# Mount Google Drive\n",
    "drive.mount('/content/drive')\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "eb701df2",
   "metadata": {
    "id": "eb701df2"
   },
   "source": [
    "# Install the google-cloud-bigquery library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12118cff",
   "metadata": {
    "id": "12118cff"
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "pip install google-cloud-bigquery"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "c86a5ae6",
   "metadata": {
    "id": "c86a5ae6"
   },
   "source": [
    "# Install upgraded bigquery library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa861f70",
   "metadata": {
    "id": "aa861f70"
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "pip install --upgrade google-cloud-bigquery"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "d426e43a",
   "metadata": {
    "id": "d426e43a"
   },
   "source": [
    "# Install pandas gcsfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e51b76b4",
   "metadata": {
    "id": "e51b76b4"
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "pip install pandas gcsfs"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "16deb34c",
   "metadata": {
    "id": "16deb34c"
   },
   "source": [
    "# Install pyarrow library\n",
    "\n",
    "**NOTE: Once pyarrow is installed, you should be able to use the load_table_from_dataframe function without encountering the ValueError from the \"Load Data into BigQuery Tables\" Cell.**\n",
    "\n",
    "**After installing pyarrow, you might need to restart your Python environment or Jupyter Notebook kernel before running the script again to ensure that the changes take effect.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "972b7202",
   "metadata": {
    "id": "972b7202"
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "pip install pyarrow"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "479a2e8f",
   "metadata": {
    "id": "479a2e8f"
   },
   "source": [
    "# Import the Python 'os' module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10a62f7d",
   "metadata": {
    "id": "10a62f7d"
   },
   "outputs": [],
   "source": [
    "# CREATE A GOOGLE ACCESSKEY\n",
    "import os\n",
    "os.environ['GOOGLE_APPLICATION_CREDENTIALS'] = '/content/drive/MyDrive/Omakasi/GOOGLE_CLOUD_ACCESSKEY.json'"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "X5nBO-xZRNE8",
   "metadata": {
    "id": "X5nBO-xZRNE8"
   },
   "source": [
    "# Install pyarrow library\n",
    "\n",
    "**NOTE: Once pyarrow is installed, you should be able to use the load_table_from_dataframe function without encountering the ValueError from the \"Load Data into BigQuery Tables\" Cell.**\n",
    "\n",
    "**After installing pyarrow, you might need to restart your Python environment or Jupyter Notebook kernel before running the script again to ensure that the changes take effect.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5sO7XkbmRNE-",
   "metadata": {
    "id": "5sO7XkbmRNE-"
   },
   "outputs": [],
   "source": [
    "pip install pyarrow"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "gsVSIZT-RNE_",
   "metadata": {
    "id": "gsVSIZT-RNE_"
   },
   "source": [
    "# Import the Python 'os' module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20f73254",
   "metadata": {
    "id": "20f73254"
   },
   "outputs": [],
   "source": [
    "from google.cloud import bigquery"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "773fb2ce",
   "metadata": {
    "id": "773fb2ce"
   },
   "source": [
    "# Create BigQuery Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d63eaaf",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 403
    },
    "id": "2d63eaaf",
    "outputId": "076a70c8-c18f-47b0-9e5f-699bdb6d044a"
   },
   "outputs": [],
   "source": [
    "from google.cloud import bigquery\n",
    "\n",
    "# Creating a function for creating a BigQuery dataset with your file stored in your Google Cloud\n",
    "def create_bigquery_dataset(project_id, dataset_name):\n",
    "    bigquery_client = bigquery.Client(project=project_id)\n",
    "    dataset_id = f\"{project_id}.{dataset_name}\"\n",
    "    dataset = bigquery.Dataset(dataset_id)\n",
    "    dataset.location = \"US\"\n",
    "    bigquery_client.create_dataset(dataset)\n",
    "    print(f\"Dataset {dataset_id} created.\")\n",
    "\n",
    "\n",
    "project_id = 'your_project_id'\n",
    "dataset_name = 'your_dataset_name'\n",
    "create_bigquery_dataset(project_id, dataset_name)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "794c0917",
   "metadata": {
    "id": "794c0917"
   },
   "source": [
    "# Create Tables in BigQuery"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eDQw87RMUByq",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "eDQw87RMUByq",
    "outputId": "beba03c9-be65-4cc2-9046-35eef5bbe028"
   },
   "outputs": [],
   "source": [
    "from google.cloud import bigquery\n",
    "from google.oauth2 import service_account\n",
    "\n",
    "# Getting the path to the service account key file from the environment variable\n",
    "service_account_path = os.environ.get('GOOGLE_APPLICATION_CREDENTIALS')\n",
    "\n",
    "# Setting your Google Cloud credentials using the environment variable\n",
    "credentials = service_account.Credentials.from_service_account_file(service_account_path)\n",
    "# Initializing a BigQuery client\n",
    "client = bigquery.Client(credentials=credentials, project=credentials.project_id)\n",
    "\n",
    "# Defining your dataset and table names\n",
    "dataset_name = 'jsoncme'\n",
    "fact_table_name = 'travel_fact'\n",
    "date_dim_table_name = 'dim_date'\n",
    "weather_dim_table_name = 'dim_weather'\n",
    "airport_dim_table_name = 'dim_airport'\n",
    "\n",
    "# Creating the dataset\n",
    "dataset_ref = client.dataset(dataset_name)\n",
    "client.get_dataset(dataset_ref)\n",
    "\n",
    "# Defining the schema for the fact table\n",
    "fact_table_schema = [\n",
    "    bigquery.SchemaField('travelid', 'INTEGER', mode='REQUIRED'),\n",
    "    bigquery.SchemaField('date_id', 'INTEGER'),\n",
    "    bigquery.SchemaField('airportid', 'INTEGER'),\n",
    "    bigquery.SchemaField('weatherid', 'INTEGER'),\n",
    "    bigquery.SchemaField('total', 'INTEGER'),\n",
    "    bigquery.SchemaField('temperature_2m', 'FLOAT'),\n",
    "    bigquery.SchemaField('precipitation', 'FLOAT'),\n",
    "    bigquery.SchemaField('relative_humidity_2m', 'FLOAT'),\n",
    "    bigquery.SchemaField('wind_speed_10m', 'FLOAT')\n",
    "]\n",
    "\n",
    "# Defining the schema for the date dimension table\n",
    "date_dim_table_schema = [\n",
    "    bigquery.SchemaField('date_id', 'INTEGER', mode='REQUIRED'),\n",
    "    bigquery.SchemaField('datetime', 'DATE'),\n",
    "    bigquery.SchemaField('day', 'INTEGER'),\n",
    "    bigquery.SchemaField('month', 'INTEGER'),\n",
    "    bigquery.SchemaField('year', 'INTEGER')\n",
    "]\n",
    "# Defining the schema for the weather table\n",
    "weather_dim_table_schema = [\n",
    "    bigquery.SchemaField('weatherid', 'INTEGER', mode='REQUIRED'),\n",
    "    bigquery.SchemaField('city', 'STRING'),\n",
    "    bigquery.SchemaField('weather_code', 'FLOAT')\n",
    "]\n",
    "\n",
    "# Defining the schema for the airport dimension table\n",
    "dim_airport_schema = [\n",
    "    bigquery.SchemaField('airportid', 'INTEGER', mode='REQUIRED'),\n",
    "    bigquery.SchemaField('airportcode', 'STRING'),\n",
    "    bigquery.SchemaField('state', 'STRING'),\n",
    "]\n",
    "\n",
    "# Creating the FACT_TABLE:\n",
    "fact_table_ref = dataset_ref.table(fact_table_name)\n",
    "try:\n",
    "    client.get_table(fact_table_ref)\n",
    "    print(f\"Table {fact_table_name} already exists in the dataset {dataset_name}.\")\n",
    "except:\n",
    "    fact_table = bigquery.Table(fact_table_ref, schema=fact_table_schema)\n",
    "    client.create_table(fact_table)\n",
    "    print(f\"{fact_table_name} Created\")\n",
    "\n",
    "# Creating the DIM_DATE table:\n",
    "date_dim_table_ref = dataset_ref.table(date_dim_table_name)\n",
    "try:\n",
    "    client.get_table(date_dim_table_ref)\n",
    "    print(f\"Table {date_dim_table_name} already exists in the dataset {dataset_name}.\")\n",
    "except:\n",
    "    date_dim_table = bigquery.Table(date_dim_table_ref, schema=date_dim_table_schema)\n",
    "    client.create_table(date_dim_table)\n",
    "    print(f\"{date_dim_table_name} Created\")\n",
    "\n",
    "# Creating the DIM_WEATHER table:\n",
    "weather_dim_table_ref = dataset_ref.table(weather_dim_table_name)\n",
    "try:\n",
    "    client.get_table(weather_dim_table_ref)\n",
    "    print(f\"Table {weather_dim_table_name} already exists in the dataset {dataset_name}.\")\n",
    "except:\n",
    "    weather_dim_table = bigquery.Table(weather_dim_table_ref, schema=weather_dim_table_schema)\n",
    "    client.create_table(weather_dim_table)\n",
    "    print(f\"{weather_dim_table_name} Created\")\n",
    "\n",
    "# Creating the airport_dim_table:\n",
    "airport_dim_table_ref = dataset_ref.table(airport_dim_table_name)\n",
    "try:\n",
    "    client.get_table(airport_dim_table_ref)\n",
    "    print(f\"Table {airport_dim_table_name} already exists in the dataset {dataset_name}.\")\n",
    "except:\n",
    "    airport_dim_table = bigquery.Table(airport_dim_table_ref, schema=weather_dim_table_schema)\n",
    "    client.create_table(airport_dim_table)\n",
    "    print(f\"{airport_dim_table_name} Created\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "47e481a4",
   "metadata": {
    "id": "47e481a4"
   },
   "source": [
    "# Read a dataset from your Google Cloud Storage into a Pandas DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3c2a360",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 394
    },
    "id": "f3c2a360",
    "outputId": "33624172-df50-4d50-b2ba-549b5ef20e5f"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from gcsfs import GCSFileSystem\n",
    "\n",
    "# Replace with you actual bucket and file path\n",
    "gcs_bucket = 'jsoncme'\n",
    "gcs_file_path = 'Final_Merged.csv'\n",
    "\n",
    "# Using Pandas to read the dataset from GCS into a DataFrame\n",
    "df = pd.read_csv(f'gcs://{gcs_bucket}/{gcs_file_path}')\n",
    "\n",
    "# Displaying the first few rows of the DataFrame\n",
    "df.head()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "aab34628",
   "metadata": {
    "id": "aab34628"
   },
   "source": [
    "# Load Data into BigQuery Tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4546ebfe",
   "metadata": {
    "id": "4546ebfe"
   },
   "outputs": [],
   "source": [
    "# Creating a function that uploads your data to BigQuery from a DataFrame\n",
    "def upload_data_from_dataframe(df, table_ref):\n",
    "    job_config = bigquery.LoadJobConfig()\n",
    "    job_config.write_disposition = bigquery.WriteDisposition.WRITE_TRUNCATE\n",
    "    job_config.autodetect = True\n",
    "    job = client.load_table_from_dataframe(df, table_ref, job_config=job_config)\n",
    "    job.result()\n",
    "\n",
    "# Splitting your DataFrame into the respective dimension and fact DataFrames\n",
    "# fact_df, date_dim_df, airport_dim_df, weather_dim_df = split_your_dataframe(df_transformed)\n",
    "def split_df(df):\n",
    "    fact_cols = [\n",
    "    \"travelid\", \"date_id\", \"airportid\",\n",
    "    \"weatherid\", \"total\",\n",
    "    \"temperature_2m\", \"precipitation\",\n",
    "    \"relative_humidity_2m\", \"wind_speed_10m\"]\n",
    "\n",
    "    date_cols = [\n",
    "    \"date_id\", \"datetime\", \"day\", \"month\", \"year\"]\n",
    "\n",
    "\n",
    "    weather_cols = [\n",
    "    \"weatherid\", \"city\", \"weather_code\"]\n",
    "\n",
    "    airport_cols = [\n",
    "    \"airportid\", \"airportcode\", \"state\"]\n",
    "\n",
    "    fact_df = df[fact_cols]\n",
    "    date_dim_df = df[date_cols]\n",
    "    weather_dim_df = df[weather_cols]\n",
    "    airport_dim_df = df[airport_cols]\n",
    "\n",
    "    # Returning the split DataFrames\n",
    "    return fact_df, date_dim_df, weather_dim_df, airport_dim_df\n",
    "\n",
    "fact_df, date_dim_df, weather_dim_df, airport_dim_df = split_df(df)\n",
    "\n",
    "# Uploading the data to BigQuery\n",
    "upload_data_from_dataframe(fact_df, fact_table_ref)\n",
    "upload_data_from_dataframe(date_dim_df, date_dim_table_ref)\n",
    "upload_data_from_dataframe(weather_dim_df, weather_dim_table_ref)\n",
    "upload_data_from_dataframe(airport_dim_df, airport_dim_table_ref)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
